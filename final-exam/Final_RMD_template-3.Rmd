---
title: "EDS241 final exam"
author: "Linus Ghanadan"
date: "2-17-24"
output: 
  pdf_document:
    toc: false
    number_sections: yes
header-includes:
  \setlength{\parindent}{1em}
  \usepackage{float}
  \renewcommand{\thesubsection}{Question (\alph{subsection})}
--- 

Make sure to read through the setup in markdown. Remember to write out interpretations and report your results in writing/ table/plot forms.

``` {r setup, echo = FALSE, message = FALSE, warning = FALSE}
#Clean Environment
rm(list=ls())

# Setup your coding process in a way that works for you. 
# Ideally use projects to organize your scripts and outputs. 
# You all probably know more about this than us! 
# For this project, I would create a project with all your data and scripts. 
# I often store data on servers rather than my computer which is why I use the code you see below.

# I set an extension to retrieve data from a particular place (Google Drive/servers etc) 
# and projects to organize my scripts and outputs on my computer/github.

# here I am setting a path to where I stored the data for this assignment
# data_wd <- "/Users/elliottfinn/Library/CloudStorage/GoogleDrive-elliottfinn@ucsb.edu/Shared drives/EDS241/Assignments/Assignment 2" 

# Example of how I use this Data Working Directory:
# data <- read_csv(paste0(data_wd,"/FILE_NAME.csv")) 
# This helps me download/manage my data from different places.

# set default chunk options
knitr::opts_chunk$set(fig.width = 4, fig.height = 3, 
                      echo = TRUE, message = FALSE, warning = FALSE)

# load packages
packages=c(
  # Necessary for Assignment 2
  "Match","plm", "tidyverse", "MatchIt", "RItools", "Hmisc", "lmtest", "estimatr",
  
  # You decide what works for you, these are the packages I use to display results 
  # they may not be the ones you use.
  "gridExtra", "stargazer", "kableExtra", 
  "purrr", "knitr", "broom", "here",
  
  # Some Potentially useful packages from earlier examples
           "stargazer", "here","stringr", "janitor", 
           "cowplot", "ggplot2", "tinytex", "datasets", "tibble") # Used for Mock assignment

for (i in packages) {
  if (require(i,character.only=TRUE)==FALSE) {
    install.packages(i,repos='http://cran.us.r-project.org')
  }
  else {
    require(i,character.only=TRUE)
  }
}

# Disable scientific notation if you want
options(scipen=999)

```

# Part 1: RCTs, treatment ignorability (selection on observables), propensity scores _(15 points total)_

**Setup**

This exercise is inspired by Costello et al. 2008 article in science “Can Catch Shares Prevent Fisheries Collapse”, which we also discussed in class (lecture 5). “Inspired” means that the data final_fisheries_data.csv are synthetically generated to simplify things for our purposes. It contains the variables on 11,135 fisheries (only cross sectional, no time observations): These fisheries were either regulated by an Individual Transferable Quota (ITQ) for all years between 1990 and 2012 or in none of those years. Variables in the dataset include:

**The outcome and treatment variables are:**

\indent COLL_SHARE = share of years a fishery is collapsed between 1990 and 2012 (collapse defined as harvest being more than 10% below maximum recorded harvest).

\indent ITQ = dummy variable indicating ‘treatment’ with an ITQ (equal to 1 if the fishery has been regulated by an ITQ and 0 otherwise). 

**The control variables are:**

\indent MET1, MET2, ….MET6 = Dummy variables indicating to which Marine Ecosystem Type (MET) the fishery belongs to (coral reefs, kelp forests, seagrass meadows, open ocean, deep sea, mangrove forests). This type does not change over the relevant time period and does not depend on human influence.

\indent IND_SR = Index of species richness in 1980 with values between 0 and 100 indicating the biodiversity with respect to species in the fishery. Bounds of 0 and 100 are the lowest and highest observed values of species diversity across all fisheries in 1980, respectively.

\indent COMM_VAL = Commercial value of fisheries in 1980 in million US-$

The basic question of interest is “What is the average treatment effect of implementing an ITQ in the time period from 1990 to 2012 on the share of years with a collapse. It is likely that the probability a fishery is selected for an ITQ depends on the pre-treatment characteristics given. It is also quite likely that the pre-treatment characteristics have an effect on the share of collapse for each fishery, i.e. our outcome variable of interest.

```{r , include=TRUE}
rm(list=ls()) # clean environment

## Load Data
fisheries_df <- read.csv(here::here("data", "final_fisheries_data.csv"))

## Prepare Data
# Change all column names to lowercase
colnames(fisheries_df) <- tolower(colnames(fisheries_df))

```
## Pretreatment Ecosystem Characteristic Comparison, Visual _(3 pts)_
(a) Compare the distributions of pre-treatment ecosystem characteristics (i.e. MET1, MET2, ,,, MET6) between the treated and the control groups by drawing back to back histograms [2 pts]. Write one sentence discussing the (dis)similarity between the two groups [1pt].

```{r}
# create MET column
fisheries_df <- fisheries_df %>%
  mutate(met = case_when(
    met1 == 1 ~ 1,
    met2 == 1 ~ 2,
    met3 == 1 ~ 3,
    met4 == 1 ~ 4,
    met5 == 1 ~ 5,
    met6 == 1 ~ 6))

# Convert MET to factors
fisheries_df$met <- as.factor(fisheries_df$met)

# Aggregate data and negate count for control
fisheries_summary <- fisheries_df %>%
  count(met, itq) %>%
  spread(key = itq, value = n) %>%
  gather(key = itq, value = 'count', -met) %>% 
  mutate(count = if_else(itq == '0', -count, count))

# Create histograms
ggplot(fisheries_summary, aes(x = met, y = count, fill = as.factor(itq))) +
  geom_col() +
  scale_y_continuous(labels = function(x) abs(x), limits = c(-2000, 2000)) +
  coord_flip() +
  labs(title = "Covariates comparison", x = "MET", y = "Count") +
  scale_fill_brewer(palette = "Set2", name = "ITQ",
                    labels = c("0" = "control", "1" = "treatment")) +
  theme_minimal()

```

[Compared to fisheries in the control group (not regulated by an ITQ), fisheries in the treatment group (regulated by an ITQ) were less likely to be coral reefs or mangrove forests and more likely to be kelp forests, seagrass meadows, open ocean, or deep sea (these differences are particularly pronounced for open ocean and deep sea).]{style="color:navy;"}

## Pretreatment Ecosystem Characteristic Comparison, Mean differences _3 pts)_
(b) Do a test on mean differences between the treated and control groups for the species richness index (IND_SR) and commercial value (COMM_VAL) variables. Interpret the results (estimated difference and significance) [2 pts] and make a conclusion regarding the similarity between the groups [1pt]. 

```{r , include=TRUE}
# calculate mean difference for IND_SR
mean_diff_ind_sr <- mean(fisheries_df$ind_sr[fisheries_df$itq == 1]) - mean(fisheries_df$ind_sr[fisheries_df$itq == 0])
paste("mean difference for IND_SR:", mean_diff_ind_sr)
# mean difference t-test for IND_SR
t.test(ind_sr ~ itq, data = fisheries_df)

# calculate mean difference for COMM_VAL
mean_diff_comm_val <- mean(fisheries_df$comm_val[fisheries_df$itq == 1]) - mean(fisheries_df$comm_val[fisheries_df$itq == 0])
paste("mean difference for COMM_VAL:", mean_diff_comm_val)
# mean difference t-test for COMM_VAL
t.test(comm_val ~ itq, data = fisheries_df)

```

[For species richness (SR) index in 1980, we reject the null hypothesis, at an alpha level of <0.001, that the difference in the mean SR index between the control (not regulated by an ITQ) and treatment (regulated by an ITQ) groups is zero. We estimate this mean difference to be about 8.83 index units (where the mean SR index is lower in the treatment group), and our 95% confidence interval for the mean difference is from 8.41 to 9.24 index units.]{style="color:navy;"}

[Regarding the commercial value of fisheries in 1980, we also reject the null hypothesis, at an alpha level of <0.001, that the difference in the mean SR index between the control (not regulated by an ITQ) and treatment (regulated by an ITQ) groups is zero. We estimate this mean difference to be about 32.35 million USD (where the mean commercial value is lower in the treatment group), and our 95% confidence interval for the mean difference is from 30.74 to 33.95 million USD.]{style="color:navy;"}

[Thus, both of our tests indicate that there are significant differences in the pre-treatment characteristics of the control (not regulated by an ITQ) and treatment (regulated by an ITQ) fisheries. On average, fisheries that were regulated by an ITQ were both significantly less biodiverse in 1980 and significantly less commercially valueable in 1980, compared to fisheries not regulated by an ITQ.]{style="color:navy;"}

## Treatment Ignorability _(1 pt)_
(c) Based on your results from (a) and (b), do you see a problem with just comparing the outcome variable means between treated and untreated fisheries? 

[Yes, I do see a problem with just comparing the outcome variable (share of years between 1990 and 2012 where harvest was more than 10% below maximum recorded harvest) means between the treated (regulated by an ITQ) and untreated (not regulated by an ITQ) fisheries because of the statistically significant differences in the covariates between these two groups that we would not be controlling for. Our histograms from Part A show differences in the count of fisheries in the treated and untreated groups across the six marine ecosystem types, and these differences were particularly pronounced for ecosystems that were classified as open ocean or deep sea (both appear to be statistically significant). In addition, our two t-tests from Part B indicate, with high statistical significance (p<0.001), mean differences between the two groups for the biodiversity in 1980 and commercial value in 1980. These covariates are likely to be determinants of whether a fishery collapses, so it would be very misleading to just compare the outcome variable means of the treatment and untreated fisheries, since there are likely to be effects from the differences in covariate variables that will be misrepresented as effects from the treatment variable.]{style="color:navy;"}


## Propensity Scores _(2 pts)_
(d) Estimate the propensity scores (probability of being treated) using a logit model, assume that all covariates are relevant and should be included in the estimation [0.5 pt]. Draw separate histograms (back to back) of the propensity scores for the treated and the untreated group [0.5 pt]. Comment on the overlap, do you have any concerns? Why/why not? [1]
```{r , include=TRUE}
## Propensity Score Estimates

# Estimation of propensity scores with the glm function and logit model
ps_reg <- glm(itq ~ met1 + met2 + met3 + met4 + met5 + ind_sr + comm_val,
          data = fisheries_df, family = binomial())

# Attach the predicted propensity score	to the data frame
fisheries_df$psvalue <- predict(ps_reg, type	= "response")

# Drawing back to back histograms for propensity scores for treated and non-treated before matching
histbackback(split(fisheries_df$psvalue, fisheries_df$itq),
             main = "Propensity score before matching", xlab = c("control", "treatment"), xlim = c(-600, 800))
```

[There is some overlap in the distributions of our histograms, mainly for fisheries that have propensity scores between 0.4 and 0.6. I'm concerned about the lack of overlap for fisheries with high propensity scores, as this may lead to issues when we do nearest-neighbor matching. During nearest-neighbor matching, treated units are assigned to the non-treated unit with the closest propensity score as a match, and we want the resulting set of fisheries to have a great deal of overlap in their propensity scores to ensure a balance in covariates, since they are also likely to have an influence on our outcome variable. However, since there are very few fisheries in the control group with high propensity scores, I'm concerned that even after nearest-neighbor matching occurs, there will still be a lack of overlap in the distributions of our histograms.]{style="color:navy;"}


## ATT with Nearest Neighbor Matching _(3 pts: 2 pt estimate, 1 pt interpretation)_
(e) Use the propensity scores from (d) to estimate the Average Treatment Effect on the Treated (ATT) with a nearest neighbor matching estimator. Interpret the result (just the size of the estimate)
```{r , include=TRUE}
## Nearest Neighbor Matching

# Match using	nearest-neighbor approach (treated units are assigned the non-treated unit with the closest propensity score as match)
nn_matching <- matchit(itq ~ met1 + met2 + met3 + met4 + met5 + ind_sr + comm_val,
                       data = fisheries_df, method = "nearest", ratio = 1)
match_data = match.data(nn_matching)

## Estimate ATT

# Calculate sum of the differences of outcomes between matches
sumdiff_data <- match_data %>%
  group_by(subclass) %>%
  mutate(diff = coll_share[itq==1] - coll_share[itq==0])
sumdiff <- sum(sumdiff_data$diff)/2

# Divide sum of the difference by the number treated to generate ATT estimate
ATT_nn = sumdiff / sum(match_data$itq)
ATT_nn

```

[Using the matched data, we estimate the average effect of an ITQ on a fishery that had an ITQ to be a 7.13 percentage-point decrease in the share of years between 1990 and 2012 where the fishery experienced collapse. This estimated effect size only applies to fisheries that were regulated by an ITQ because we only defined counterfactuals for these fisheries. Thus, our average outcome only demonstrates the effect of an ITQ on fishery collapse as it pertains to fisheries that were regulated by an ITQ.]{style="color:navy;"}


## ATE with WLS _(3 pts: 1 pt estimate, 1 pt interpretation)_
(f) Estimate the Average Treatment Effect (ATE) using the weighted least squares on the full sample. Interpret the estimated size and conclude if it is significantly different from zero from a statistical perspective.
```{r , include=TRUE}
## WLS Matching

# calculation of the weights (see slide 25 of lecture 5)
PS <- fisheries_df$psvalue
D <- fisheries_df$itq

fisheries_df$wgt = (D/PS + (1-D)/(1-PS))

#### Weighted least Squares (WLS) Estimates
reg_wls	<-lm(coll_share ~ itq + met1 + met2 + met3 + met4 + met5 + ind_sr + comm_val,
               data = fisheries_df, weights = wgt)

## Estimate ATE

# Extracting the coefficients table
summary_reg <- summary(reg_wls)
summary_reg$coefficients %>% 
  kbl(caption = "WLS estimates") %>%  # Generate table
  kable_classic(full_width = FALSE)

```

[Using the WLS estimator, we estimate the average effect of an ITQ on a fishery to be a 7.67 percentage-point decrease in the share of years between 1990 and 2012 where the fishery experienced collapse, with a standard error of 0.099 percentage-points. This estimated effect size applies to all fisheries because we did not remove any fisheries from our sample as we did when estimating effect size based on matching. Our very low p-value means we reject the null hypothesis that the difference is zero at an alpha level of <0.001.]{style="color:navy;"}

# Part 2 Difference in Difference Estimation _(10 points total + 3pts extra credit)_

\indent Here we return for a final time to the dataset from Gertler, Martinez, and Rubio-Codina (2012) and use a different way of estimating the effect of the Mexican conditional cash transfer on the value of animal holdings of recipients. We’ll use the panel data from assignment 2, where you have both the pre-program and post-program observations. See Template for dataset preparation instructions.

\indent **Data Preparation**

\indent *Note: You will need to install the packages plm and dplyr (included in template preamble). Again, you can find a description of the variables at the bottom of PDF and HERE.

Prepare Data: Load the new data (progresa_pre_1997.csv) and the follow-up data (progresa_post_1999.csv) into R. Note that we created a time denoting variable (with the same name, 'year') in BOTH datasets. Again, you will create a panel dataset by appending the data (i.e. binding the dataset row-wise together creating a single dataset). We want to examine the same outcome variable as before, value of family animal holdings (vani). You will use the full dataset for each estimate. NOTE: you should not change any NAs from the TREATED column in your analysis, as we expect that spillover was likely in this program. NAs will be excluded from your calculations/estimations.

```{r , include=TRUE, echo=FALSE}
rm(list=ls()) # clean environment

## Load/Prep Data
# How I setup my coding flow
# Keep project WD, set extension to get data from google drive 
# data_wd <- "/Users/elliottfinn/Library/CloudStorage/GoogleDrive-elliottfinn@ucsb.edu/Shared drives/EDS241/Assignments/Assignment 2"

# Load 1997 and 1999 Progresa datasets

## Load the datasets
progresa_pre_1997 <- read_csv(here("data", "progresa_pre_1997.csv"))
progresa_post_1999 <- read_csv(here("data", "progresa_post_1999.csv"))

####### CODE FIX FOR FINAL ######

### Append post to pre dataset 
progresa_full <- rbind(progresa_pre_1997, progresa_post_1999) # same as original (note, you can keep NAs in the data- they'll be excluded from any estimates etc)
progresa_full <- progresa_full %>%
 group_by(hhid) %>% filter(n() == 2) %>%
 ungroup()

# This removes all families lost to attrition, 
# in other words. Families who were treated/controls in the program, but did not get measured
# in the second year. This can happen for many reasons with observational data, you often
# lose participants as studies go on for longer periods of time.

rm(progresa_pre_1997, progresa_post_1999) # clean unused data

```
## DiD Estimator, ATE _(5 pts: 3 pts estimate, 2 pts interpretation)_
(a) Calculate the DiD estimator of the treatment effect (ATE) of the program on the value of animal holdings (vani)  “manually” i.e. based on group mean values without running a regression. Report and interpret the result (Note: no significance test or standard errors is possible, so you do not need to report these values).
```{r, include=TRUE}
## Estimate ATE with DiD estimator manually

# Calculate mean vani in treatment group before and after program
mean_treat_pre <- mean(progresa_full$vani[progresa_full$treatment == 1 & progresa_full$year == 1997], na.rm = TRUE)
mean_treat_post <- mean(progresa_full$vani[progresa_full$treatment == 1 & progresa_full$year == 1999], na.rm = TRUE)

# Calculate mean vani in control group before and after program
mean_control_pre <- mean(progresa_full$vani[progresa_full$treatment == 0 & progresa_full$year == 1997], na.rm = TRUE)
mean_control_post <- mean(progresa_full$vani[progresa_full$treatment == 0 & progresa_full$year == 1999], na.rm = TRUE)

# Calculate DiD estimator
(mean_treat_post - mean_treat_pre) - (mean_control_post - mean_control_pre)

```

[From our manual calculation of the DiD estimator, we estimate the average effect of the cash transfer program on value of animal holdings to be about 287.91 USD (where program participants had a higher average value of animal holdings at the end of the program). To interpret the result in this way, we are assuming that our control (units that did not participate in program) and treatment (program participants) group satisfy the parallel trends assumption. This means that we're assuming that if the program did not exist, the mean value of animal of animal holdings in the units that were in the treatment group would have changed at the same rate as the units in the control group (i.e., we're assuming that the control group provides a valid counterfactual).]{style="color:navy;"}

## Difference in Difference using OLS _(5 pts)_
(b) Now set up an OLS-regression using group mean values to estimate the same ATE. Interpret the estimated treatment effect [3 pts]. Also interpret the coefficients on the time dummy and the group dummy variable (see interpretation done in class in lecture 9) [2 pts]. 

\indent **Hints:** You will need to create a new dataframe with a variety of dummy variables to do this. The R example provided with the DiD module (and/or the excel file) should help.

```{r}
# Create a new data frame for OLS regression
ols_df <- data.frame(
  treatment_dummy = c(1, 1, 0, 0),
  post_treatment_time_dummy = c(0, 1, 0, 1),
  vani_mean = c(mean_treat_pre, mean_treat_post, mean_control_pre, mean_control_post))

# Run the OLS regression
ols_reg_group_means <- lm(vani_mean ~ treatment_dummy + post_treatment_time_dummy + post_treatment_time_dummy*treatment_dummy, data = ols_df)

# Present Regressions in Table
summary_reg <- summary(ols_reg_group_means)
summary_reg$coefficients %>% 
  kbl(caption = "DiD ATE estimation with group mean values") %>%  # Generate table
  kable_classic(full_width = FALSE)

```

[From our regression using group mean values, we estimate the average effect of the cash transfer program on value of animal holdings to be about 287.91 USD. Just like in Part A, we have to assume that the control group (units that did not participate in program) provides a valid counterfactual for what would have happened to units in our treatment group (program participants) had they not participated in the program to interpret this result as the average treatment effect.]{style="color:navy;"}

[The coefficient on our treatment dummy variable tells us that we estimate the mean difference in the outcome variable (value of animal holdings) between the treatment group (program participants) and the control group (non-participants) before the program started to have been 237.69 USD (where program participants had a lower average value of animal holdings than non-participants prior to the start of the program).]{style="color:navy;"}

[The coefficient on our post treatment time dummy variable tells us that we estimate the mean change in the outcome variable (value of animal holdings) between the beginning and end of the program for the control group (non-participants in program) to be 1,156.75 USD (where non-participants had a lower average value of animal holdings when the program ended than when it started).]{style="color:navy;"}


# Extra Credit: ATE with OLS using full dataset _(3 pts: 2 pts estimate, 1 pt interpretation)_
(c) Estimate the ATE with an OLS-regression based on the original units as observations (i.e. not with group mean values, you will need to use the entire dataset). Even though the specification is the same as in the regression with the group mean values above, you’ll need to create new indicator variables for the treatment group and the post treatment time period as well as their interaction term. Verify that you get the same result as above. Now report also on the precision of the estimation and test whether the estimated coefficient is different from zero. 

```{r, include=TRUE}
## Create the dummy variables (you'll need 3)
progresa_full$treatment_dummy <- ifelse(progresa_full$treatment == 1, 1, 0)
progresa_full$post_treatment_time_dummy <- ifelse(progresa_full$year == 1999, 1, 0)
progresa_full$interaction_dummy <- progresa_full$treatment_dummy * progresa_full$post_treatment_time_dummy

## OLS regression
ols_reg_all_values <- lm(vani ~ treatment_dummy + post_treatment_time_dummy + interaction_dummy, data = progresa_full)

# Present Regressions in Table
summary_reg <- summary(ols_reg_all_values)
summary_reg$coefficients %>% 
  kbl(caption = "DiD ATE estimation with all values") %>%  # Generate table
  kable_classic(full_width = FALSE)

```

[From our regression using the full dataset, we estimate the average effect of the cash transfer program on value of animal holdings to be about 287.91 USD, with a standard error of about 113.78 USD. Just like in Part A and Part B, we have to assume that the control group (units that did not participate in program) provides a valid counterfactual for what would have happened to units in our treatment group (program participants) had they not participated in the program to interpret this result as the average treatment effect. Furthermore, our p-value of 0.011 tells us that, at an alpha level of 0.05, we reject the null hypothesis that there was no average effect of the cash transfer program on value of animal holdings.]{style="color:navy;"}

[The coefficient on our treatment dummy variable tells us that we estimate the mean difference in the outcome variable (value of animal holdings) between the treatment group (program participants) and the control group (non-participants) before the program started to have been 237.69 USD (where program participants had a lower average value of animal holdings than non-participants prior to the start of the program), with a standard error of 80.45 USD. Our p-value of <0.01 tells us that, at an alpha level of 0.01, we reject the null hypothesis that there was a mean difference of zero.]{style="color:navy;"}

[The coefficient on our post treatment time dummy variable tells us that we estimate the mean change in the outcome variable (value of animal holdings) between the beginning and end of the program for the control group (non-participants in program) to be 1,156.75 USD (where non-participants had a lower average value of animal holdings when the program ended than when it started), with a standard error of 85.72 USD. Our p-value of 0.003 tells us that, at an alpha level of 0.01, we reject the null hypothesis that there was a mean change of zero.]{style="color:navy;"}

